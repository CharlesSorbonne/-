{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from Parser import Parser\n",
    "from IndexerSimple import IndexerSimple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME 1 Indexation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explication : \n",
    "\n",
    "$\\verb|Document.py|$ : définir la classe $\\verb|Document|$ (la classe enregistre les données des documents), on va l'utiliser dans la classe $\\verb|Parser|$ <br/>\n",
    "\n",
    "$\\verb|Parser.py|$ : définir la classe $\\verb|Parser|$ (la classe lit tous les documents enregistré dans un fichier)<br/>\n",
    "\n",
    "$\\verb|IndexerSimple.py|$ : définir la classe $\\verb|IndexerSimple|$ (la classe calcule les indexs d'après un collection de type dictionnaire de la classe $\\verb|Document|$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- TEST ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection1 = Parser.Parser_doc('./data/cacmShort-good.txt')\n",
    "indexer=IndexerSimple(collection1)\n",
    "indexer.indexation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index et index inverse\n",
    "on remaque que $\\verb|nbDocument==len(index)|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index,indexInverse = indexer.index,indexer.indexInverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preliminari': 1.791759469228055,\n",
       " 'report': 1.791759469228055,\n",
       " 'intern': 1.791759469228055,\n",
       " 'algebra': 1.791759469228055,\n",
       " 'languagejgkfldjgfkld': 1.791759469228055}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.getTfIDFsForDoc(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 1.0986122886681098, 7: 1.0986122886681098, 10: 1.0986122886681098}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.getTfIDFsForStem('terminolog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "Identifiant : 1\n",
      "Titre : Preliminary Report-International Algebraic Languagejgkfldjgfkld\n",
      "Auteurs : \n",
      "\tPerlis A. J.\n",
      "\tSamelson K.\n",
      "Nombre de liens : 34\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer.getStrDoc(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME 2 Appariement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explication : \n",
    "\n",
    "$\\verb|ModeleScoreSimple.py|$ : définir la classe $\\verb|ModeleScore|$ (la classe calcule modèle booléen et modèle vectoriel simplement et one ne l'utilise jamais) <br/>\n",
    "\n",
    "$\\verb|Weighter.py|$ : définir la classe $\\verb|Weighter|$ et $\\verb|Weighter1-5|$ (la classe calcule les pondérations pour le modèle vectoriel)<br/>\n",
    "\n",
    "$\\verb|IRModel.py|$ : définir la classe $\\verb|IRModel|$ (abstrait) et \n",
    "> $\\verb|Vectoriel|$<br/>\n",
    "> $\\verb|Okapi|$ <br/>\n",
    "> $\\verb|ModeleLangue|$\n",
    "\n",
    "(la classe calcule la score d'après un paramètre de type $\\verb|Weighter|$ (on peut utilise index dans $\\verb|Weighter|$) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- TEST ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModeleScoreSimple import ModeleScore\n",
    "req = \"computer terminology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modèle booléen  :  {4: 1, 7: 1, 10: 1}\n",
      "modele vectoriel:  {2: 1, 4: 2, 6: 1, 7: 2, 10: 2}\n",
      "modèle booléen  (utilise indexInverse):  {10: 1, 4: 1, 7: 1}\n"
     ]
    }
   ],
   "source": [
    "modeleScore = ModeleScore(collection1,req)\n",
    "\n",
    "print (\"modèle booléen  : \",modeleScore.modelBool())\n",
    "print (\"modèle vectoriel: \",modeleScore.modelVector())\n",
    "\n",
    "print (\"modèle booléen  (utilise indexInverse): \",modeleScore.modelBool_indexInverse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation : <br/>\n",
    "- $w_{t,d}$ est le poids d'un terme $t$ dans un document $d$ et $w_{t,q}$ le poids d'un terme $t$ dans une requete $q$.<br/>\n",
    "- $tf_{t,d}$ (resp. $tf_{t,q}$) correspond au *term frequency* du terme $t$  dans le document $d$ (resp. la requete $q$).<br/>\n",
    "- $idf_t$ correspond à l'*inverse document frequency* du terme $t$ dans l'ensemble de la collection/du corpus considéré<br/>\n",
    "\n",
    "$\\verb|weight1|$ : $w_{t,d} = tf_{t,d}$ et $w_{t,q} = 1$ si $t \\in q$ <br/>\n",
    "$\\verb|weight2|$ : $w_{t,d} = tf_{t,d}$ et $w_{t,q} = tf_{t,q}$ si $t \\in q$<br/>\n",
    "$\\verb|weight3|$ : $w_{t,d} = tf_{t,d}$ et $w_{t,q} = idf_t$ si $t \\in q$<br/>\n",
    "$\\verb|weight4|$ : $w_{t,d} = 1 + ln(tf_{t,d})$ et $w_{t,q} = idf_t$ si $t \\in q$<br/>\n",
    "$\\verb|weight4|$ : $w_{t,d} = (1 + ln(tf_{t,d})) \\times idf_t$ et $w_{t,q} = (1 + ln(tf_{t,q})) \\times idf_t$ si $t \\in q$<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightsForQuery : \n",
      "\t {'comput': 0.6931471805599453, 'terminolog': 1.0986122886681098}\n",
      "WeightsForDoc : \n",
      "\t {'preliminari': 1.791759469228055, 'report': 1.791759469228055, 'intern': 1.791759469228055, 'algebra': 1.791759469228055, 'languagejgkfldjgfkld': 1.791759469228055}\n",
      "WeightsForStem : \n",
      "\t {1: 1.791759469228055}\n"
     ]
    }
   ],
   "source": [
    "import Weighter\n",
    "\n",
    "w=Weighter.Weighter5(indexer)\n",
    "\n",
    "print (\"WeightsForQuery : \\n\\t\",w.getWeightsForQuery(req))\n",
    "print (\"WeightsForDoc : \\n\\t\",w.getWeightsForDoc(1))\n",
    "print (\"WeightsForStem : \\n\\t\",w.getWeightsForStem(\"interne\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle vectoriel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarque : (modele vectoriel)\n",
    "\n",
    "Le poids d'un terme n'appartenant pas à la requete sera toujours nul. <br/>\n",
    "\n",
    "Donc, le produit scalaire entre le vecteur de la requete et un vecteur de document ne prendra pas en compte les termes ne se trouvant pas dans la requete (multiplication par 0).<br/>\n",
    "\n",
    "Ainsi, on ne retournera pas les documents ayant un score nul (rapidité d'execution). La norme de chaque vecteur sera calculée la première fois que cela est nécessaire et sera gardée en mémoire pour la suite.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  {2: 0.09473140616367136, 4: 0.588769560363494, 6: 0.14079185956679588, 7: 0.588769560363494, 10: 0.588769560363494}\n",
      "classement :  [4, 7, 10, 6, 2]\n"
     ]
    }
   ],
   "source": [
    "from IRModel import *\n",
    "\n",
    "v=Vectoriel(w, True)\n",
    "\n",
    "print (\"score : \",v.getScores(req))\n",
    "print (\"classement : \",v.getRanking(req))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle probabiliste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  {2: 0.6187041659413354, 4: 1.7360571023090483, 6: 0.8101157672794361, 7: 1.7360571023090483, 10: 1.7360571023090483}\n",
      "classement :  [4, 7, 10, 6, 2]\n"
     ]
    }
   ],
   "source": [
    "w=Weighter.Weighter1(indexer)\n",
    "o=Okapi(w)\n",
    "\n",
    "print (\"score : \",o.getScores(req))\n",
    "print (\"classement : \",o.getRanking(req))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle de langues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  {2: 0.0017993079584775079, 4: 0.030850288350634376, 6: 0.0033679354094578997, 7: 0.030850288350634376, 10: 0.030850288350634376}\n",
      "classement :  [4, 7, 10, 6, 2]\n"
     ]
    }
   ],
   "source": [
    "w=Weighter.Weighter1(indexer)\n",
    "RSV=ModeleLangue(w)\n",
    "\n",
    "print (\"score : \",RSV.getScores(req))\n",
    "print (\"classement : \",RSV.getRanking(req))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explication : \n",
    "\n",
    "$\\verb|Query.py|$ : définir la classe $\\verb|Query|$ (la classe enregistre les données des requetes), on va l'utiliser dans la classe $\\verb|QueryParser|$ (très proche que $\\verb|Document|$)<br/>\n",
    "\n",
    "$\\verb|QueryParser.py|$ : définir la classe $\\verb|QueryParser|$ (la classe lit tous les requetes enregistré dans un fichier) (très proche que $\\verb|Parser|$)<br/>\n",
    "\n",
    "$\\verb|EvalMesure.py|$ : définir la classe $\\verb|EvalMesure|$ (abstrait) et \n",
    "> $\\verb|Precision|$ <br/>\n",
    "$\\verb|Rappel|$ <br/>\n",
    "$\\verb|F-mesure|$ <br/>\n",
    "$\\verb|NDCG|$ <br/>\n",
    "$\\verb|AP|$ <br/>\n",
    "$\\verb|RR|$ \n",
    "\n",
    "(la classe calcule tous les mesure d'évaluation) <br/>\n",
    "\n",
    "$\\verb|EvalIRModel.py|$ : définir la classe $\\verb|EvalIRModel|$ (la classe permet l'évaluation de différents modèles de recherche sur un ensemble de requetes selon différentes mesures d'évaluation)\n",
    "\n",
    "$\\verb|GridSearch.py|$ : définir la classe $\\verb|GridSearch|$ (la classe permet l'évaluation de différents paramètres de un modèle de recherche sur un ensemble de requetes selon un mesure d'évaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- TEST ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger des requêtes et de leur docs pertinents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QueryParser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1410, 1572, 1605, 2020, 2358]\n",
      " What articles exist which deal with TSS (Time Sharing System), an\n",
      "operating system for IBM computers?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req = QueryParser('./data/cacm/cacm.qry','./data/cacm/cacm.rel')\n",
    "req1 = req.parseQRY()\n",
    "req2 = req.parseREL()\n",
    "print (req2[1].docsPertinents) # req2[1] ou bien req.reqs[1]\n",
    "print (req2[1].texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métriques et Plateforme d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EvalMesure import *\n",
    "from EvalIRModel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectoriel  :\n",
      "\tWeight  1 ; mesure  Precision \n",
      "\t\tmean:  0.052000000000000005 std:  0.03709447398198282\n",
      "\tWeight  1 ; mesure  Rappel \n",
      "\t\tmean:  0.3076190476190476 std:  0.2547770355987382\n",
      "\tWeight  1 ; mesure  F_mesure \n",
      "\t\tmean:  0.0794089565629021 std:  0.0445619246991046\n",
      "\tWeight  1 ; mesure  AP \n",
      "\t\tmean (MAP):  0.08550502169087199 std:  0.055100354020198423\n",
      "\tWeight  1 ; mesure  NDCG \n",
      "\t\tmean:  0.21320321263610564 std:  0.28766103146071925\n",
      "\tWeight  1 ; mesure  RR \n",
      "\t\tmean (MRR):  0.2887770562770563 std:  0.29182370159167503\n",
      "\tWeight  2 ; mesure  Precision \n",
      "\t\tmean:  0.05600000000000001 std:  0.04543126676640219\n",
      "\tWeight  2 ; mesure  Rappel \n",
      "\t\tmean:  0.3520238095238095 std:  0.2790116840586964\n",
      "\tWeight  2 ; mesure  F_mesure \n",
      "\t\tmean:  0.08620849455536747 std:  0.053516756782500334\n",
      "\tWeight  2 ; mesure  AP \n",
      "\t\tmean (MAP):  0.11672996065437447 std:  0.10917264677569836\n",
      "\tWeight  2 ; mesure  NDCG \n",
      "\t\tmean:  0.12982462285909657 std:  0.16376680469897711\n",
      "\tWeight  2 ; mesure  RR \n",
      "\t\tmean (MRR):  0.3384387351778656 std:  0.3584530563656995\n",
      "Okapi  :\n",
      "\tWeight  1 ; mesure  Precision \n",
      "\t\tmean:  0.084 std:  0.08475848040166836\n",
      "\tWeight  1 ; mesure  Rappel \n",
      "\t\tmean:  0.43742063492063493 std:  0.29230257047445085\n",
      "\tWeight  1 ; mesure  F_mesure \n",
      "\t\tmean:  0.12359805082827371 std:  0.09593265025080626\n",
      "\tWeight  1 ; mesure  AP \n",
      "\t\tmean (MAP):  0.15350891982746412 std:  0.15492977534612348\n",
      "\tWeight  1 ; mesure  NDCG \n",
      "\t\tmean:  0.27285245835429883 std:  0.33675697555894846\n",
      "\tWeight  1 ; mesure  RR \n",
      "\t\tmean (MRR):  0.44000000000000006 std:  0.39074003861618506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Vectoriel-Precision-W1': (0.052000000000000005, 0.03709447398198282),\n",
       " 'Vectoriel-Rappel-W1': (0.3076190476190476, 0.2547770355987382),\n",
       " 'Vectoriel-F_mesure-W1': (0.0794089565629021, 0.0445619246991046),\n",
       " 'Vectoriel-AP-W1': (0.08550502169087199, 0.055100354020198423),\n",
       " 'Vectoriel-NDCG-W1': (0.21320321263610564, 0.28766103146071925),\n",
       " 'Vectoriel-RR-W1': (0.2887770562770563, 0.29182370159167503),\n",
       " 'Vectoriel-Precision-W2': (0.05600000000000001, 0.04543126676640219),\n",
       " 'Vectoriel-Rappel-W2': (0.3520238095238095, 0.2790116840586964),\n",
       " 'Vectoriel-F_mesure-W2': (0.08620849455536747, 0.053516756782500334),\n",
       " 'Vectoriel-AP-W2': (0.11672996065437447, 0.10917264677569836),\n",
       " 'Vectoriel-NDCG-W2': (0.12982462285909657, 0.16376680469897711),\n",
       " 'Vectoriel-RR-W2': (0.3384387351778656, 0.3584530563656995),\n",
       " 'Okapi-Precision-W1': (0.084, 0.08475848040166836),\n",
       " 'Okapi-Rappel-W1': (0.43742063492063493, 0.29230257047445085),\n",
       " 'Okapi-F_mesure-W1': (0.12359805082827371, 0.09593265025080626),\n",
       " 'Okapi-AP-W1': (0.15350891982746412, 0.15492977534612348),\n",
       " 'Okapi-NDCG-W1': (0.27285245835429883, 0.33675697555894846),\n",
       " 'Okapi-RR-W1': (0.44000000000000006, 0.39074003861618506)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalIRModel = EvalIRModel('./data/cacm/cacm')\n",
    "evalIRModel.fit(weighters=[Weighter1,Weighter2],k=50)\n",
    "evalIRModel.evaluation(nameModels=[\"Vectoriel\",\"Okapi\"],nbQuery =10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GridSearch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi  : parametre optimal =  (1.3, 0.85)  \n",
      "\tmean(Train) =  0.15290570175277668\n",
      "--- Test ---\n",
      "Okapi  :\n",
      "\tWeight  1 ; mesure  F_mesure \n",
      "\t\tmean:  0.00392156862745098 std:  0.011764705882352941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Okapi-F_mesure-W1': (0.00392156862745098, 0.011764705882352941)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridSearch = GridSearch('./data/cacm/cacm',percent=0.75)\n",
    "gridSearch.fit()\n",
    "gridSearch.evaluation (nameModels=[\"Okapi\"], nameMesures=[\"AP\"], weighters=[Weighter1], nbQuery = 10, k = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explication : \n",
    "\n",
    "$\\verb|PageRank.py|$ : définir la classe $\\verb|PageRank|$ (la classe calcule PageRank) <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- TEST ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PageRank import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converge en  8  iteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[925, 175, 1302, 603, 245, 484, 382, 625, 1390, 413]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/cisi/cisi.txt'   \n",
    "n = 20\n",
    "k = 10\n",
    "query = \"the present time\"\n",
    "pr = PageRank (path,model=None)\n",
    "pr.PageRank (query,n,k )[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "2e5c6628eef985e7fd2fa2aad22c988c5b8aa1d2648cf9c51c543a2a2637c546"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
